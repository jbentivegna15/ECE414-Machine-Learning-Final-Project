{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ML Final Project Kaggle Competition\n",
    "#By Jeremiah Pratt and Joseph Bentivegna\n",
    "#Dedicated to my mother, who always withheld affection and told me I'd never make anything of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import robust_scale\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainyBoi = pd.read_csv('train.csv',header=None)\n",
    "testyBoi = pd.read_csv('test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n",
      "0.59\n",
      "0.545\n",
      "0.555\n",
      "0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeremiah.v.pratt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "scaler = MinMaxScaler()\n",
    "X = trainyBoi.iloc[:,1:]\n",
    "X = scaler.fit_transform(X)\n",
    "Y = trainyBoi.iloc[:,0]\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "logreg = linear_model.LogisticRegression(penalty='l2',C=10,solver='newton-cg',max_iter=1000,tol=1e-5)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    logreg.fit(X_train,Y_train)\n",
    "    print(logreg.score(X_test,Y_test))\n",
    "#print(metrics.confusion_matrix(Y_test,logreg.predict(X_test)))\n",
    "\n",
    "logreg.fit(X,Y)\n",
    "testyBoi = scaler.fit_transform(testyBoi)\n",
    "pred = pd.DataFrame(logreg.predict(testyBoi))\n",
    "pred.to_csv('results.csv',index_label=['Id','Prediction'],index=list(range(1,200)))\n",
    "\n",
    "#Playing with feature selection, doesn't really make it better\n",
    "X = trainyBoi.iloc[:,1:]\n",
    "sel = MinMaxScaler()\n",
    "poly = PolynomialFeatures(2)\n",
    "varthresh = VarianceThreshold(threshold=(.99*(1-.99)))\n",
    "X = sel.fit_transform(X)\n",
    "X = poly.fit_transform(X)\n",
    "X = varthresh.fit_transform(X)\n",
    "avg = 0\n",
    "logreg = linear_model.LogisticRegression(penalty='l2',C=50,solver='sag',max_iter=10000,tol=1e-7,multi_class = 'multinomial')\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    logreg.fit(X_train,Y_train)\n",
    "    print(logreg.score(X_test,Y_test))\n",
    "    avg = avg + logreg.score(X_test,Y_test)\n",
    "print(avg/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried Stochastic Gradient Descent\n",
    "scaler = MinMaxScaler()\n",
    "poly = PolynomialFeatures(2)\n",
    "varthres = VarianceThreshold(threshold = (.99*(1-.99)))\n",
    "X = trainyBoi.iloc[:,1:]\n",
    "X = scaler.fit_transform(X)\n",
    "X = poly.fit_transform(X)\n",
    "Y = trainyBoi.iloc[:,0]\n",
    "X = varthres.fit_transform(X)\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "avg = 0\n",
    "SGD = linear_model.SGDClassifier(loss=\"log\",penalty=\"l2\",tol=1e-5,max_iter=1000000,fit_intercept=False,alpha=.0001)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    SGD.fit(X_train,Y_train)\n",
    "    print(SGD.score(X_test,Y_test))\n",
    "    avg = avg + SGD.score(X_test,Y_test)\n",
    "#print(metrics.confusion_matrix(Y_test,SGD.predict(X_test)))\n",
    "print(avg/5)\n",
    "SGD.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Playing with feature selection, doesn't really make it better\n",
    "X = trainyBoi.iloc[:,1:]\n",
    "Y = trainyBoi.iloc[:,0]\n",
    "sel = MinMaxScaler()\n",
    "X=sel.fit_transform(X)\n",
    "gammas = np.linspace(1/20,1/1000)\n",
    "cs = np.linspace(.5,50)\n",
    "tols = [1e-9, 5e-8, 1e-8, 5e-7, 1e-7, 5e-6,1e-6, 5e-5, 1e-5, 5e-4,1e-4,5e-3,1e-3,5e-2,1e-2,5e-1,1e-1,1]\n",
    "bestavg = 0\n",
    "bestc = 1000000\n",
    "besttol = 1000000\n",
    "bestgam = 10000000\n",
    "totalRuns = len(gammas)*len(cs)*len(tols)\n",
    "runs = 0\n",
    "for i in range(len(gammas)):\n",
    "    for j in range(len(cs)):\n",
    "        for k in range(len(tols)):\n",
    "            clf = svm.SVC(C=cs[j],gamma=gammas[i], decision_function_shape='ovr',tol=tols[k])\n",
    "            kf = KFold(n_splits=20, shuffle=True)\n",
    "            avg = 0\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "                clf.fit(X_train,Y_train)\n",
    "                #print(logreg.score(X_test,Y_test))\n",
    "                avg = avg + clf.score(X_test,Y_test)\n",
    "\n",
    "            avg = avg/20\n",
    "            if(avg>bestavg):\n",
    "                bestc = cs[j]\n",
    "                besttol = tols[k]\n",
    "                bestgam = gammas[i]\n",
    "                bestavg = avg\n",
    "\n",
    "            runs = runs+1\n",
    "            print(\"percent done:\")\n",
    "            print(runs/totalRuns)\n",
    "\n",
    "print(bestavg)\n",
    "print(besttol)\n",
    "print(bestc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
